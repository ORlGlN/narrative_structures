sparse_tdm[1:20,1:8]
class(machtdm)
class(sparse_tdm)
machtdm[1:20,1:8]
sparse_tdm[1:20,1:8]
dim(sparse_tdm) # What are the dimensions of the matrix? Rows, Columns
object.size(sparse_tdm) # How many bytes?
colnames(sparse_tdm) # What are the column names?
machRawtdm[1:20, 1:8]
machtdm
machtdm[1:20,1:8]
inspect(machRawtdm[1:20, 1:8])
inspect(machRawtdm[1:100, 1:8])
start=Sys.time() # This is a simple timing mechanism. Creates a variable that saves the current time and...
space <- svds(sparse_tdm, 100)
# 	easyspace = lsa(machtdm, dims=100) # This will give a warning - last dimension calculated is very close to zero size
Sys.time()-start # ... this line shows the now-current time minus the "start" time.
class(space) # A list
str(space) #          with five objects inside it
object.size(space)
dim(space)
tk <- space$d * space$u
dk <- space$d * Matrix::t(space$v)
dimnames(tk) <- list(dimnames(sparse_tdm)[[1]], 1:100)
dimnames(dk) <- list(1:100, dimnames(sparse_tdm)[[2]])
dim(tk) # Dimensions of term matrix
dim(dk) # Dimensions of doc matrix
length(space$d) # Number of singular values
tk[1:10,1:10]
round(tk[1:10,1:10], 1)
round(tk[1:10,1:10], 2)
findFreqTerms(machRawtdm, lowfreq=100) # Words which occur 100+ times
neighbors("trust", 30, tvectors=tk[,1:100])
neighbors("trust", 30, tvectors=tk[,1:30])
plot_neighbors("trust", 30, dims=2, tvectors=tk, breakdown=F)
plot_neighbors("trust", 30, dims=3, tvectors=tk, breakdown=F)
charLength = lapply(colnames(dk), nchar)
colnames(dk)[which(charLength==5)] = paste("00", colnames(dk)[which(charLength==5)], sep="")
colnames(dk)[which(charLength==6)] = paste("0", colnames(dk)[which(charLength==6)], sep="")
start=Sys.time()
paraCos = multicos(sort(colnames(dk)), tvectors=t(dk), breakdown=F)
Sys.time()-start
paraCos[1:10,1:10] # Look at first 10x10 of matrix.
write.csv(paraCos, file="Mach_paraCos.csv")
heatmap(paraCos, symm=T, main="Machiavelli Clustered Paras")
plot(space$d)
rTerms = sample(rownames(tk), 30) # random sample of term names
plot(tk[rTerms,1],tk[rTerms,2])
text(tk[rTerms,1],tk[rTerms,2], labels=rTerms)
costring("trust", "useful perfect", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust queen", "useful sin", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust queen", "elizabeth judgment", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince souldiours", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince fear", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince love", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince pikes", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince pope", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince souldiours", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "pope", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "state fear", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "state love", tvectors= tk[,1:50], breakdown=TRUE)
termlist = c("souldiours", "power", "prince", "princes", "state", "warre", "pikes", "enemies", "trust", "love", "fear", "pope", "alexander") # Create a list of specified terms to use for particular investigation.
specificTermCos = multicos(termlist, tvectors= tk[,1:50], breakdown=F)
specificTermCos
Term_count <-apply(tk,1,sum)
TCT <- t(Term_count)
myTerms <- rownames(tk)
str(myTerms)
TCT
plot_neighbors("prince", 30, dims=3, tvectors=tk, breakdown=F)
heatmap(paraCos, symm=T, Rowv=NA,  main="Machiavelli Ordered Paras")
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(pROC)
require(stringr)
require(splitstackshape)
require(ez)
require(FactoMineR)
library(MASS)
library(readability)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(pROC)
require(stringr)
require(splitstackshape)
require(ez)
require(FactoMineR)
library(MASS)
library(readability)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(stringr)
require(ez)
library(MASS)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
path = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/second_run_vectorized_results.txt'
annot = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/best_friend/best_friend_parsed_annotations.csv'
lines = read.table(path, header=FALSE, sep=",")
annot_lines = read.table(annot, header=FALSE, sep=",")
View(annot_lines)
View(lines)
names(annot_lines) = c(
'id',
'veracity',
'plausibility'
)
names(lines) = c(
'id'
, 'veracity'
, 'k1'
, 'k2'
, 'k3'
, 'k4'
, 'k5'
, 'k6'
, 'k7'
, 'k8'
, 'k9'
, 'k10'
)
merged = merge(lines, annot_lines, all.x = TRUE, by = c("id"))
View(merged)
path = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/second_run_vectorized_results.txt'
annot = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/best_friend/best_friend_parsed_annotations.csv'
lines = read.table(path, header=FALSE, sep=",")
annot_lines = read.table(annot, header=FALSE, sep=",")
path = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/second_run_vectorized_results.txt'
annot = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/best_friend_parsed_annotations.csv'
lines = read.table(path, header=FALSE, sep=",")
annot_lines = read.table(annot, header=FALSE, sep=",")
names(annot_lines) = c(
'id',
'veracity',
'plausibility'
)
names(lines) = c(
'id'
, 'veracity'
, 'k1'
, 'k2'
, 'k3'
, 'k4'
, 'k5'
, 'k6'
, 'k7'
, 'k8'
, 'k9'
, 'k10'
)
merged = merge(lines, annot_lines, all.x = TRUE, by = c("id"))
View(merged)
cor.test(merged$k9, merged$plausibility, method="spearm", alternative="greater")
cor.test(~ k9 + plausibility,
data=merged,
method = "spearman",
continuity = FALSE,
conf.level = 0.95)
require(pROC)
roc(response = c(merged$veracity.x)
, predictor = c(merged$k10)
, ci=T)
truthful = subset(merged$k2, merged$veracity.x == 'truthful')
deceptive = subset(merged$k2, merged$veracity.x == 'deceptive')
mean(truthful)
mean(deceptive)
t.test(merged$k9 ~ merged$veracity.x, paired=T, alternative = 'less')
source("/Users/bennettkleinberg/Documents/Research/analysis/R_script/dz_within_ci.R")
merged$veracity.x
dz_within_ci(merged$k9[merged$veracity.x == 'truthful']
, merged$k9[merged$veracity.x == 'deceptive']
, report = T)
source("/Users/bennettkleinberg/Documents/Research/analysis/R_script/ds_between_ci.R")
parallel_cluster
install.packages("aws.s3")
library(aws.s3)
setwd('/Users/bennettkleinberg/Documents/Research/aws/intro')
# trees is a built-in data set. Let's create a new CSV file that we can upload
# to AWS S3.
write.csv(trees, "trees.csv", row.names = FALSE)
setwd('/Users/bennettkleinberg/Documents/Research/aws/intro')
library(aws.s3)
# Set up your keys and your region here.
Sys.setenv("AWS_ACCESS_KEY_ID" = "AKIAIEY2BOJFWZHUP2JA",
"AWS_SECRET_ACCESS_KEY" = "5+55DBvZVrwQJ0xBWw0qZwD6OuzZ9C6125hTH01H",
"AWS_DEFAULT_REGION" = "us-east-2")
sample(c(0:9, letters), size = 10, replace = TRUE)
# Coming up with unique bucket names is tricky, so we'll add a random string
# to the name of this bucket.
bucket_name = paste(c("data_repo_", sample(c(0:9, LETTERS), size = 8, replace = TRUE)), sep='')
bucket_name
paste(c("data_repo_", sample(c(0:9, LETTERS), size = 8, replace = TRUE)), collapse = "")
# Coming up with unique bucket names is tricky, so we'll add a random string
# to the name of this bucket.
bucket_name = paste(c("data_repo_", sample(c(0:9, LETTERS), size = 12, replace = TRUE)), collapse = "")
bucket_name
# Now we can create the bucket.
put_bucket(bucket_name)
# Coming up with unique bucket names is tricky, so we'll add a random string
# to the name of this bucket.
bucket_name = paste(c("data_repo_", sample(c(0:9, letters), size = 10, replace = TRUE)), collapse = "")
# Now we can create the bucket.
put_bucket(bucket_name)
# Coming up with unique bucket names is tricky, so we'll add a random string
# to the name of this bucket.
bucket_name = paste(c("datarepo-", sample(c(0:9, letters), size = 10, replace = TRUE)), collapse = "")
# Now we can create the bucket.
put_bucket(bucket_name)
# Let's put our CSV file in the bucket.
put_object("trees.csv", bucket = bucket_name)
# We've put data in The Cloud! Now let's get it back on our computer:
save_object("trees.csv", bucket = bucket_name, file = "trees_s3.csv")
# Are the files the same?
trees_s3 = read.csv("trees_s3.csv")
all.equal(trees, trees_s3)
# We're finished with this bucket, so let's delete it.
delete_bucket(bucket_name)
install.packages("cloudyr")
install.packages("aws.ec2metadata")
##########################
#EC2
install.packages("aws.ec2", repos = c(getOption("repos"), "http://cloudyr.github.io/drat"))
##########################
#EC2
library(aws.ec2)
library(aws.ec2metadata)
###############################################################################
##### NARRATIVE STRUCTURE OF VLOGS ON YOUTUBE
##### KLEINBERG, MOZES, VAN DER VEGT ###
###############################################################################
###############################################################################
### ANALYSIS
### FOR PREPROCESSING SEE narr_structure_1.R
###############################################################################
# PREPARATION
## clear ws
rm(list = ls())
## load deps
require(tm)
require(data.table)
require(tidyr)
require(syuzhet)
require(tidyverse)
require(cluster)
require(factoextra)
require(parallel)
require(rlist)
require(quanteda)
setwd('/Users/bennettkleinberg/GitHub/r_helper_functions')
source('./get_narrative_dim.R')
setwd('/Users/bennettkleinberg/Documents/Research/UCL/narrative_structures_local/data/output_dir')
#START ANALYSIS
load('vlogs_data_07032018.RData')
df.sentiment = get_narrative_dim_min(txt_input_col = df.data$text[1:20]
, txt_id_col = df.data$channel_vlog_id[1:20]
, method = 'sentimentr'
, transform_values = T
)
df.sentiment
df.sentiment = get_narrative_dim_min(txt_input_col = df.data$text[1:100]
, txt_id_col = df.data$channel_vlog_id[1:100]
, method = 'sentimentr'
, transform_values = T
)
df.sentiment
rm(df.sentiment)
###############################################################################
##### NARRATIVE STRUCTURE OF VLOGS ON YOUTUBE
##### KLEINBERG, MOZES, VAN DER VEGT ###
###############################################################################
###############################################################################
### ANALYSIS
### FOR PREPROCESSING SEE narr_structure_1.R
###############################################################################
# PREPARATION
## clear ws
rm(list = ls())
## load deps
require(tm)
require(data.table)
require(tidyr)
require(syuzhet)
require(tidyverse)
require(cluster)
require(factoextra)
require(parallel)
require(rlist)
require(quanteda)
setwd('/Users/bennettkleinberg/GitHub/r_helper_functions')
source('./get_narrative_dim.R')
## set dir
#setwd('/Users/bennettkleinberg/GitHub/narrative_structures/data/output_dir')
setwd('/Users/bennettkleinberg/GitHub/narrative_structures/data/output_dir')
setwd('/Users/bennettkleinberg/Documents/Research/UCL/narrative_structures_local/data/output_dir')
#START ANALYSIS
load('vlogs_data_07032018.RData')
rm(df.full_corpus, df.meta)
ls()
#non-paralle
nrow(data)
#non-paralle
nrow(df.data)
#non-paralle
nrow(df.data)/4000
#non-paralle
nrow(df.data)/400
#non-paralle
nrow(df.data)/410
#non-paralle
nrow(df.data)/1002
#non-paralle
nrow(df.data)/40
#non-paralle
nrow(df.data)/30
#non-paralle
nrow(df.data)/25
#non-parallel
batch = 1:1000
batch = 1:10
df.sentiment = get_narrative_dim_min(txt_input_col = df.data$text[batch]
, txt_id_col = df.data$channel_vlog_id[batch]
, method = 'sentimentr'
, transform_values = T
)
min(batch)
paste('batch', min(batch), max(batch), sep="_")
savename = paste(paste('batch', min(batch), max(batch), sep="_"), '.RData', sep="")
savename
batches = c(batch1, batch2)
batch1 = 1:10
batch2 = 11:20
batches = c(batch1, batch2)
batches
seq_batch = seq(1,nrow(df.data), 1000)
seq_batch
seq_batch = seq(1,nrow(df.data), 1000)
for(i in seq_batch){
new_index = i:i+1
which(i == seq_batch)
}
seq_batch = seq(1,nrow(df.data), 1000)
for(i in seq_batch){
new_index = i:i+1
print(new_index)
which(i == seq_batch)
}
seq_batch = seq(1,nrow(df.data), 1000)
for(i in seq_batch){
new_index = seq_batch[i]:seq_batch[i+1]
print(new_index)
which(i == seq_batch)
}
for(i in seq_batch){
# new_index = seq_batch[i]:seq_batch[i+1]
# print(new_index)
print(which(i == seq_batch))
}
length(seq_batch)
seq_batch = seq(1,nrow(df.data), 1000)
for(i in seq_batch){
seq_index = which(i == seq_batch)
if(seq_index < length(seq_batch)){
new_index = seq_batch[i]:seq_batch[i+1]
} else {
new_index = seq_batch[i]:nrow(df.data)
}
print(new_index)
}
i = 1
new_index = seq_batch[i]:seq_batch[i+1]
i = 2
new_index = seq_batch[i]:seq_batch[i+1]
i = 42
new_index = seq_batch[i]:seq_batch[i+1]
i = 43
new_index = seq_batch[i]:seq_batch[i+1]
#non-parallel
seq_batch = seq(1,nrow(df.data), 1000)
seq_batch
length(seq_batch)
seq_batch = seq(1,nrow(df.data), 1000)
for(i in seq_batch){
seq_index = which(i == seq_batch)
print(seq_index)
if(seq_index < length(seq_batch)){
new_index = seq_batch[i]:seq_batch[i+1]
} else {
new_index = seq_batch[i]:nrow(df.data)
}
print(new_index)
}
seq_index = 2
seq_index < length(seq_batch)
new_index = seq_batch[i]:seq_batch[i+1]
seq_batch[i]:
seq_batch[i]:seq_batch[i+1]
seq_batch = seq(1,nrow(df.data), 1000)
for(i in seq_batch){
seq_index = which(i == seq_batch)
if(seq_index < length(seq_batch)){
new_index = seq_batch[seq_index]:seq_batch[seq_index+1]
} else {
new_index = seq_batch[seq_index]:nrow(df.data)
}
print(new_index)
}
seq_batch = seq(1,nrow(df.data), 1000)
for(i in seq_batch){
print(i)
seq_index = which(i == seq_batch)
if(seq_index < length(seq_batch)){
new_index = seq_batch[seq_index]:seq_batch[seq_index+1]
} else {
new_index = seq_batch[seq_index]:nrow(df.data)
}
print(new_index)
}
min(new_index)
new_index
df.sentiment
rm(df.sentiment)
df.sentiment
#non-parallel
###simple loop
seq_max = nrow(df.data)/22
seq_max
#non-parallel
###simple loop
getwd()
###############################################################################
##### NARRATIVE STRUCTURE OF VLOGS ON YOUTUBE
##### KLEINBERG, MOZES, VAN DER VEGT ###
###############################################################################
###############################################################################
### ANALYSIS
### FOR PREPROCESSING SEE narr_structure_1.R
###############################################################################
# PREPARATION
## clear ws
rm(list = ls())
## load deps
require(tm)
require(data.table)
require(tidyr)
require(syuzhet)
require(tidyverse)
require(cluster)
require(factoextra)
require(parallel)
require(rlist)
require(quanteda)
setwd('/Users/bennettkleinberg/GitHub/r_helper_functions')
source('./get_narrative_dim.R')
## set dir
#setwd('/Users/bennettkleinberg/GitHub/narrative_structures/data/output_dir')
setwd('/Users/bennettkleinberg/GitHub/narrative_structures/data/output_dir')
setwd('/Users/bennettkleinberg/Documents/Research/UCL/narrative_structures_local/data/output_dir')
#START ANALYSIS
load('vlogs_data_07032018.RData')
rm(df.full_corpus, df.meta)
################################################################################
########## narrative structure extraction
################################################################################
#non-parallel
###simple loop
setwd('./loop_run')
seq_max = 100
seq_batch = seq(1,nrow(df.data), 20)
seq_batch
seq_batch = seq(1,seq_max, 20)
seq_batch
for(i in seq_batch){
print(i)
seq_index = which(i == seq_batch)
if(seq_index < length(seq_batch)){
new_index = seq_batch[seq_index]:seq_batch[seq_index+1]
} else {
new_index = seq_batch[seq_index]:seq_max
}
df.sentiment = get_narrative_dim_min(txt_input_col = df.data$text[new_index]
, txt_id_col = df.data$channel_vlog_id[new_index]
, method = 'sentimentr'
, transform_values = T
)
savename = paste(paste('batch', min(new_index), max(new_index), sep="_"), '.RData', sep="")
save(df.sentiment
, file = savename)
rm(df.sentiment)
}
