knife = 480
gun = 482
stick = 475
bat = 488
all = c(knife, hammer, gun, stick, bat)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 590),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,550,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="Mean reaction time")
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="RTs")
par(mfrow=c(1,2))
png(filename = "bar_graph_NWO_05102015.png",
width = 32, height = 20, units = "cm", pointsize = 20,
bg = "white",  res = 300)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="RTs")
dev.off()
setwd("/Users/bennettkleinberg/Documents/Research/PhD Amsterdam/nwo_aanvraag")
hammer = 565
knife = 480
gun = 482
stick = 475
bat = 488
all = c(knife, hammer, gun, stick, bat)
par(mfrow=c(1,2))
png(filename = "bar_graph_NWO_05102015.png",
width = 32, height = 20, units = "cm", pointsize = 20,
bg = "white",  res = 300)
barplot(all,
xlab = "",
ylab = "",
names.arg = c("GUN", "HAMMER", "KNIFE", "STICK", "BAT"),
space = .3,
ylim = c(450, 600),
axes=F,
xpd = F,
col = c("black"),
density = c(100, 20, 100, 100, 100)
)
axis(side=2, at=seq(450,600,50), las=2)
title(#"Expected result in memory detection test.",
#xlab="Stimulus",
ylab="RTs")
dev.off()
citations = 4924
followers = 770
Fc = 43.3*(citations^0.32)
Fa = followers
Kindex = Fa/Fc
Kindex
citations = 75 #based on google scholar
followers = 1273 #twitter
Fc = 43.3*(citations^0.32)
Fa = followers
Kindex = Fa/Fc
Kindex
citations = 8379 #based on google scholar
followers = 2240 #twitter
Fc = 43.3*(citations^0.32)
Fa = followers
Kindex = Fa/Fc
Kindex
library(equivalence)
df = data.frame('a' = rnorm(100, 2, 1))
df$b = rnorm(100, 1, 1)
mean(df$a)
mean(df$b)
library(equivalence)
df = data.frame('a' = rnorm(1000, 2, 1))
df$b = rnorm(1000, 1, 1)
mean(df$a)
mean(df$b)
df = data.frame('a' = rnorm(10000, 2, 1))
df$b = rnorm(10000, 1, 1)
mean(df$a)
mean(df$b)
t.test(df$a, df$b)
tost(x = df$a
, y = df$b,
, epsilon = 2
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
tost(x = df$a
, y = df$b,
, epsilon = 1
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
tost(x = df$a
, y = df$b,
, epsilon = .1
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
mean(df$a)
mean(df$b)
tost(x = df$a
, y = df$b,
, epsilon = 1
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
tost(x = df$a
, y = df$b,
, epsilon = 2
#, paired
, var.equal = T
#, conf.level = xx
, alpha = 0.05
)
a = 1:5
b = 12:17
a
a*b
a = 1:5
b = 12:16
a*b
a%*%b
# clear ws
rm(list = ls())
# IC2S2 LSA Workshop Code
# 2017-07-10
# Jacob Miller, Drexel University: jlm479@drexel.edu
#    with assistance from Jorge Fresneda, David Gefen, James Endicott, Kai Larsen, and others in the R community
###########################
# Section 1: Set up R.
# If you have never loaded these packages in this R environment, you need to do this to download them.
# tm: Text Mining;
# LSAfun: LSA functions;
# Matrix: Sparse matrix package
# RSpectra: Fast SVD
# gplots: fancy plotting
#install.packages(c("tm", "LSAfun", "Matrix", "RSpectra", "gplots"), dependencies=TRUE)
library(tm) # This loads them into your environment so you can now use that code.
library(LSAfun)
library(Matrix)
library(RSpectra)
rm(list = ls())
library(tm) # This loads them into your environment so you can now use that code.
install.packages(c("tm", "LSAfun", "Matrix", "RSpectra", "gplots"), dependencies=TRUE)
install.packages(c("tm", "LSAfun", "Matrix", "RSpectra", "gplots"), dependencies = TRUE)
library(tm) # This loads them into your environment so you can now use that code.
library(LSAfun)
library(Matrix)
library(RSpectra)
library(gplots)
machFileDirectory = '/Users/bennettkleinberg/Documents/Research/analysis/lsa/files' # Mac/Unix style
# machFileDirectory = 'C:/Users/Owner/Desktop/Dropbox/LSA Workshop/files'
doc_source <- DirSource(machFileDirectory, recursive = TRUE)
object.size(doc_source)
raw_corpus <- VCorpus(doc_source, readerControl=list(language='en'))
omitDocNames = paste(c(452, 454, 456, 458, 460:470, 518), ".txt", sep="")
omitDocs = which(names(raw_corpus) %in% omitDocNames)
machRawtdm <- TermDocumentMatrix(raw_corpus[-omitDocs],
control = list(removePunctuation = TRUE,
bounds=list(global=c(2,Inf))))
machtdm <- TermDocumentMatrix(raw_corpus[-omitDocs],
control = list(removePunctuation = TRUE,
# stemming=TRUE,
stopwords = TRUE,
weighting = function(x)  weightTfIdf(x, normalize = FALSE),
bounds=list(global=c(2,Inf))))
machtdm
inspect(machtdm[1:20,1:8])
sparse_tdm <- Matrix::sparseMatrix(i = machtdm$i, j = machtdm$j, x = machtdm$v, dims = c(machtdm$nrow, machtdm$ncol))
dimnames(machtdm)
dimnames(sparse_tdm) <- dimnames(machtdm)
sparse_tdm[1:20,1:8]
machtdm
sparse_tdm[1:20,1:8]
class(machtdm)
class(sparse_tdm)
machtdm[1:20,1:8]
sparse_tdm[1:20,1:8]
dim(sparse_tdm) # What are the dimensions of the matrix? Rows, Columns
object.size(sparse_tdm) # How many bytes?
colnames(sparse_tdm) # What are the column names?
machRawtdm[1:20, 1:8]
machtdm
machtdm[1:20,1:8]
inspect(machRawtdm[1:20, 1:8])
inspect(machRawtdm[1:100, 1:8])
start=Sys.time() # This is a simple timing mechanism. Creates a variable that saves the current time and...
space <- svds(sparse_tdm, 100)
# 	easyspace = lsa(machtdm, dims=100) # This will give a warning - last dimension calculated is very close to zero size
Sys.time()-start # ... this line shows the now-current time minus the "start" time.
class(space) # A list
str(space) #          with five objects inside it
object.size(space)
dim(space)
tk <- space$d * space$u
dk <- space$d * Matrix::t(space$v)
dimnames(tk) <- list(dimnames(sparse_tdm)[[1]], 1:100)
dimnames(dk) <- list(1:100, dimnames(sparse_tdm)[[2]])
dim(tk) # Dimensions of term matrix
dim(dk) # Dimensions of doc matrix
length(space$d) # Number of singular values
tk[1:10,1:10]
round(tk[1:10,1:10], 1)
round(tk[1:10,1:10], 2)
findFreqTerms(machRawtdm, lowfreq=100) # Words which occur 100+ times
neighbors("trust", 30, tvectors=tk[,1:100])
neighbors("trust", 30, tvectors=tk[,1:30])
plot_neighbors("trust", 30, dims=2, tvectors=tk, breakdown=F)
plot_neighbors("trust", 30, dims=3, tvectors=tk, breakdown=F)
charLength = lapply(colnames(dk), nchar)
colnames(dk)[which(charLength==5)] = paste("00", colnames(dk)[which(charLength==5)], sep="")
colnames(dk)[which(charLength==6)] = paste("0", colnames(dk)[which(charLength==6)], sep="")
start=Sys.time()
paraCos = multicos(sort(colnames(dk)), tvectors=t(dk), breakdown=F)
Sys.time()-start
paraCos[1:10,1:10] # Look at first 10x10 of matrix.
write.csv(paraCos, file="Mach_paraCos.csv")
heatmap(paraCos, symm=T, main="Machiavelli Clustered Paras")
plot(space$d)
rTerms = sample(rownames(tk), 30) # random sample of term names
plot(tk[rTerms,1],tk[rTerms,2])
text(tk[rTerms,1],tk[rTerms,2], labels=rTerms)
costring("trust", "useful perfect", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust queen", "useful sin", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust queen", "elizabeth judgment", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince souldiours", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince fear", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince love", tvectors= tk[,1:50], breakdown=TRUE)
costring("trust", "prince pikes", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince pope", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince souldiours", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "prince", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "pope", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "state fear", tvectors= tk[,1:50], breakdown=TRUE)
costring("power", "state love", tvectors= tk[,1:50], breakdown=TRUE)
termlist = c("souldiours", "power", "prince", "princes", "state", "warre", "pikes", "enemies", "trust", "love", "fear", "pope", "alexander") # Create a list of specified terms to use for particular investigation.
specificTermCos = multicos(termlist, tvectors= tk[,1:50], breakdown=F)
specificTermCos
Term_count <-apply(tk,1,sum)
TCT <- t(Term_count)
myTerms <- rownames(tk)
str(myTerms)
TCT
plot_neighbors("prince", 30, dims=3, tvectors=tk, breakdown=F)
heatmap(paraCos, symm=T, Rowv=NA,  main="Machiavelli Ordered Paras")
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(pROC)
require(stringr)
require(splitstackshape)
require(ez)
require(FactoMineR)
library(MASS)
library(readability)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(pROC)
require(stringr)
require(splitstackshape)
require(ez)
require(FactoMineR)
library(MASS)
library(readability)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
#####START
#MODEL STATEMENT ANALYSIS ML paper
#R pipeline
#clear ws
rm(list = ls())
#load deps
require(stringr)
require(ez)
library(MASS)
#local sources
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/spacy_ner_r.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/txt_df_from_dir.R')
source('/Users/bennettkleinberg/Documents/Research/analysis/R_script/get_single_readability.R')
path = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/second_run_vectorized_results.txt'
annot = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/best_friend/best_friend_parsed_annotations.csv'
lines = read.table(path, header=FALSE, sep=",")
annot_lines = read.table(annot, header=FALSE, sep=",")
View(annot_lines)
View(lines)
names(annot_lines) = c(
'id',
'veracity',
'plausibility'
)
names(lines) = c(
'id'
, 'veracity'
, 'k1'
, 'k2'
, 'k3'
, 'k4'
, 'k5'
, 'k6'
, 'k7'
, 'k8'
, 'k9'
, 'k10'
)
merged = merge(lines, annot_lines, all.x = TRUE, by = c("id"))
View(merged)
path = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/second_run_vectorized_results.txt'
annot = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/best_friend/best_friend_parsed_annotations.csv'
lines = read.table(path, header=FALSE, sep=",")
annot_lines = read.table(annot, header=FALSE, sep=",")
path = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/second_run_vectorized_results.txt'
annot = '/Users/bennettkleinberg/GitHub/causalities_dd/src/analysis/vectorized2/best_friend/best_friend_parsed_annotations.csv'
lines = read.table(path, header=FALSE, sep=",")
annot_lines = read.table(annot, header=FALSE, sep=",")
names(annot_lines) = c(
'id',
'veracity',
'plausibility'
)
names(lines) = c(
'id'
, 'veracity'
, 'k1'
, 'k2'
, 'k3'
, 'k4'
, 'k5'
, 'k6'
, 'k7'
, 'k8'
, 'k9'
, 'k10'
)
merged = merge(lines, annot_lines, all.x = TRUE, by = c("id"))
View(merged)
cor.test(merged$k9, merged$plausibility, method="spearm", alternative="greater")
cor.test(~ k9 + plausibility,
data=merged,
method = "spearman",
continuity = FALSE,
conf.level = 0.95)
require(pROC)
roc(response = c(merged$veracity.x)
, predictor = c(merged$k10)
, ci=T)
truthful = subset(merged$k2, merged$veracity.x == 'truthful')
deceptive = subset(merged$k2, merged$veracity.x == 'deceptive')
mean(truthful)
mean(deceptive)
t.test(merged$k9 ~ merged$veracity.x, paired=T, alternative = 'less')
source("/Users/bennettkleinberg/Documents/Research/analysis/R_script/dz_within_ci.R")
merged$veracity.x
dz_within_ci(merged$k9[merged$veracity.x == 'truthful']
, merged$k9[merged$veracity.x == 'deceptive']
, report = T)
source("/Users/bennettkleinberg/Documents/Research/analysis/R_script/ds_between_ci.R")
parallel_cluster
###############################################################################
##### NARRATIVE STRUCTURE OF VLOGS ON YOUTUBE
##### KLEINBERG, MOZES, VAN DER VEGT ###
###############################################################################
###############################################################################
### ANALYSIS
### FOR PREPROCESSING SEE narr_structure_1.R
###############################################################################
# PREPARATION
## clear ws
rm(list = ls())
## load deps
require(ClusterR)
setwd('/Users/bennettkleinberg/GitHub/r_helper_functions')
source('./get_narrative_dim.R')
## set dir
setwd('/Users/bennettkleinberg/GitHub/narrative_structures/data/output_dir')
load('youtube_vlogs_sentiments.RData')
## CLUSTER ANALYSIS
df.t_sentiments = t(df.sentiments)
df.sentiments_scaled = scale(df.t_sentiments, center = T, scale = T)
load('wss_cluster_1_30.RData')
plot(1:30, wss_sum)
k_means_model = MiniBatchKmeans(df.sentiments_scaled
, clusters = 5
, batch_size = 100
, num_init = 10
, initializer = 'kmeans++'
, verbose = T
)
k_means = predict_MBatchKMeans(df.sentiments_scaled
, k_means_model$centroids)
table(k_means)
#merge to data
df.kmeans = cbind(df.t_sentiments, k_means)
names(df.kmeans)
class(df.kmeans)
#merge to data
df.kmeans = data.frame(cbind(df.t_sentiments, k_means))
class(df.kmeans)
names(df.kmeans)
table(df.kmeans$k_means)
#avg per cluster
avg_cluster = tapply(df.kmeans[, 1:100], df.kmeans$k_means, mean)
avg_cluster = aggregate(df.kmeans[, 1:100]
, by= df.kmeans$k_means, mean)
avg_cluster = aggregate(df.kmeans[, 1:100]
, by = list(df.kmeans$k_means), mean)
avg_cluster
avg_cluster = avg_cluster[, -1]
avg_cluster
avg_cluster = as.data.frame(t(avg_cluster))
avg_cluster
names(avg_cluster)
paste('cluster', 1:5, sep"_")
paste('cluster', 1:5, sep="_")
names(avg_cluster) = paste('cluster', 1:5, sep="_")
plot(avg_cluster$cluster_1)
plot(avg_cluster$cluster_1
, type='h')
plot(avg_cluster$cluster_2
, type='h')
plot(avg_cluster$cluster_3
, type='h')
plot(avg_cluster$cluster_4
, type='h')
plot(avg_cluster$cluster_5
, type='h')
#merge to data
df.kmeans = data.frame(cbind(df.sentiments_scaled, k_means))
install.packages("scales")
require(scales)
#merge to data
df.kmeans = data.frame(cbind(df.sentiments_scaled, k_means))
avg_cluster = aggregate(df.kmeans[, 1:100]
, by = list(df.kmeans$k_means), mean)
avg_cluster = avg_cluster[, -1]
avg_cluster
avg_cluster = rescale(avg_cluster, to=c(-1, 1))
rescale
